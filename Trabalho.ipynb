{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07ba5bab",
   "metadata": {},
   "source": [
    "# Neste trabalho, vamos analisar empréstimos imobiliários\n",
    "##### Nossa base de dados compõe de um arquivo zipado de 166.13 MB (que dezipado chega a 1,5Gb). Nosso objetivo é prever se um determinado cliente irá dar calote no empréstimo ou não baseado. Vamos usar métodos de Deep Learning para buscar cumprir esse objetivo. Antes disso, devemos preparar nossa base de dados, que é o que faremos inicialmente. O link para a base de dados está aqui https://www.kaggle.com/datasets/deependraverma13/house-loan-data-analysis-deep-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cec8b728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação das bibliotecas a serem utilizadas\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "import time \n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bc4f4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código extra -> Definição de padrões de fonte nas figuras apresentadas\n",
    "plt.rc('font', size=12)\n",
    "plt.rc('axes', labelsize=12, titlesize=12)\n",
    "plt.rc('legend', fontsize=12)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "\n",
    "# Código extra -> Definição de padrões de visualização dos dataframes\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3144304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código extra -> Salvar as figuras no formato .png\n",
    "IMAGES_PATH = Path() / \"images\" / \"end_to_end_project\"\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0833d100",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\danie\\\\OneDrive\\\\Área de Trabalho\\\\Mestrado\\\\Deep Learning\\\\Trabalhoarchive.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m nome_arquivo_csv \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloan_data (1).csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Extração o arquivo CSV do arquivo zip\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mZipFile(caminho_arquivo_zip, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Verificar se o arquivo CSV está presente no zip\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nome_arquivo_csv \u001b[38;5;129;01min\u001b[39;00m zip_ref\u001b[38;5;241m.\u001b[39mnamelist():\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m zip_ref\u001b[38;5;241m.\u001b[39mopen(nome_arquivo_csv) \u001b[38;5;28;01mas\u001b[39;00m csv_file:\n\u001b[0;32m     14\u001b[0m             \u001b[38;5;66;03m# Ler o arquivo CSV e armazenar na variável df\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\zipfile.py:1284\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1284\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mopen(file, filemode)\n\u001b[0;32m   1285\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   1286\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\danie\\\\OneDrive\\\\Área de Trabalho\\\\Mestrado\\\\Deep Learning\\\\Trabalhoarchive.zip'"
     ]
    }
   ],
   "source": [
    "# Leitura do arquivo\n",
    "# Caminho para o arquivo zipado\n",
    "caminho_arquivo_zip = r'C:\\Users\\danie\\OneDrive\\Área de Trabalho\\Mestrado\\Deep Learning\\Trabalhoarchive.zip' # O leitor desse documento deverá substituir esse\n",
    "# caminho por onde escolheu salvar a base de dados\n",
    "\n",
    "#Nome do arquivo CSV dentro do arquivo zip\n",
    "nome_arquivo_csv = 'loan_data (1).csv'\n",
    "\n",
    "# Extração o arquivo CSV do arquivo zip\n",
    "with zipfile.ZipFile(caminho_arquivo_zip, 'r') as zip_ref:\n",
    "    # Verificar se o arquivo CSV está presente no zip\n",
    "    if nome_arquivo_csv in zip_ref.namelist():\n",
    "        with zip_ref.open(nome_arquivo_csv) as csv_file:\n",
    "            # Ler o arquivo CSV e armazenar na variável df\n",
    "            df = pd.read_csv(csv_file,low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a9dd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Não temos informação sobre o significado de FLAGDOCUMENT ou EXTSOURCE, vamos retirar essas colunas para evitr maiores problemas\n",
    "# Retiramos também a coluna de ID, pois ela não agregará em nada no nosso modelo futuro\n",
    "df = df[df.columns.drop(list(df.filter(regex='FLAG_DOCUMENT_')))] \n",
    "df = df[df.columns.drop(list(df.filter(regex='EXT_SOURCE_')))] \n",
    "df = df.drop(['SK_ID_CURR'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e45b4",
   "metadata": {},
   "source": [
    "## Lidando com dados faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf400e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum(axis=1).value_counts().plot(\n",
    "    kind=\"bar\", title=\"Quantidade de valores faltantes\", figsize = (21,13)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8bc825",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "missing = df.isnull().sum()\n",
    "100*missing/df.shape[0] # Números em porcentagem, para facilitar a visualização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b654f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temos várias lacunas grandes no nosso dataset, por escolha, vamos excluir colunas com mais de 30% de dados faltantes.\n",
    "# Com uma base de dados tão esburacada, nosso modelo não iria performar bem nessa parte.\n",
    "missing = df.isnull().sum()\n",
    "for col in missing[missing > 0.3].index.tolist():\n",
    "    del df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a10752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos verificar o tipo de dado em cada coluna e quantos dados diferentes temos em cada coluna\n",
    "for x in df.columns:\n",
    "    print(f\"{x} --> {np.dtype(df[x]), df[x].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1be57ba",
   "metadata": {},
   "source": [
    "### Devemos perceber que, apesar de algumas colunas serem numéricas, elas são puramente categóricas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aec866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos agora separar as colunas que são categóricas das que são puramente numéricas\n",
    "# Vamos usar 10 valores distintos como ponto de corte para os casos de dúvida\n",
    "# Nossas colunas numéricas são então\n",
    "num = ['TARGET','CNT_CHILDREN','AMT_INCOME_TOTAL','AMT_CREDIT','REGION_POPULATION_RELATIVE','DAYS_BIRTH','DAYS_EMPLOYED','DAYS_REGISTRATION',\n",
    "       'DAYS_ID_PUBLISH','HOUR_APPR_PROCESS_START']\n",
    "\n",
    "# Nossas colunas de categoria, são as restantes\n",
    "cat = [col for col in df.columns.values if col not in num]\n",
    "for col in num:\n",
    "    df[col] = (df[col].astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e2e96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos ver algumas métricas estatísticas das colunas numéricas\n",
    "df[num].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513634eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vamos visualizar como alguns dados estão correlacionados com nossa variável de interesse 'TARGET' para \n",
    "## facilitar nosso entendimento\n",
    "corr_matrix = df[num].corr(numeric_only=True)\n",
    "corr_matrix[\"TARGET\"].abs().sort_values(ascending=False)*100 # Os valores abaixo estão em porcentagem!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aa1e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por simplicidade, vamos separar 5 atributos com maior correlação para uma análise mais profunda\n",
    "# Vamos selecionar apenas as colunas que possuem correlação maior que 5% em valor absoluto\n",
    "\n",
    "top5 = ['DAYS_BIRTH','DAYS_ID_PUBLISH','DAYS_EMPLOYED','DAYS_REGISTRATION','REGION_POPULATION_RELATIVE']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651e56ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise do top 5\n",
    "scatter_matrix(df[top5], figsize=(21, 15))\n",
    "save_fig(\"scatter_matrix_plot\")  # extra\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66154895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A distribuição da variável 'DAYS_EMPLOYED' está um pouco fora do comum, vamos analisá-la mais a fundo\n",
    "pd.set_option('display.max_rows', 50)\n",
    "df[\"DAYS_EMPLOYED\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661e5395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percebemos no gráfico anterior que possivelmente temos um valor anômalo em torno de 350.000\n",
    "# Nossa estratégia escolhida para lidar com valores faltantes será de substituição pela média dos outros valores\n",
    "df['DAYS_EMPLOYED_NAN'] = (df[\"DAYS_EMPLOYED\"]==365243).astype('int')\n",
    "# Vamos agora trocar esses valores por NaN, e, depois, pela sua média\n",
    "\n",
    "df['DAYS_EMPLOYED'].replace(365243.0, np.nan, inplace=True)\n",
    "media = df['DAYS_EMPLOYED'].mean()\n",
    "df['DAYS_EMPLOYED'].replace(np.nan, media , inplace=True)\n",
    "# Vamos agora ver como a correlação ficou\n",
    "df[num].corr(numeric_only = True)['TARGET'].abs().sort_values(ascending=False)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b85650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nova análise do Top 5\n",
    "scatter_matrix(df[top5], figsize=(21, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9092f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[top5].hist(bins=50, figsize=(12, 8))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
